## Research Papers in a Nutshell

### What we are trying to do?

Summarize research papers by extracting important sentences from them and using them to generate Presentations.

### Flowchart

![Text Summarization Flowchart](https://raw.githubusercontent.com/abhishah901/IR-project/master/Text%20Summarization%20detailed%20flowchart.png)

### Data

https://github.com/mahnazkoupaee/WikiHow-Dataset



ABSTRACT | PGN | LSA | LUNH | TEXTRANK
---------|-----|-----|------|-----------
The statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of a translation target text given a translation source text.According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993).Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words.There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English.Germann et al.(2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.This paper presents two decoding methods, one is the right-to-left decoding based on the left-to- right beam search algorithm, which generates outputs from the end of a sentence.The second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one.The experimental results of Japanese and English translation indicated that the right-to-left decoding was better for English-to-Japanese translation, while the left-to-right decoding was better for Japanese-to-English decoding.The above results could be justified by the structural difference of Japanese and English, where English takes the prefix structure that places emphasis at the beginning of a sentence, hence prefers left-to-right decoding.On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.The bidirectional decoding, which can take both of the benefits of decoding method, was superior to mono- directional decoding methods.The next section briefly describes the SMT focusing on the IBM Model 4.Then, the Section 3 presents decoding algorithms in three direction, left- to-right, right-to-left and bi-direction.The Section 4 presents the results of Japanese and English translation followed by discussions.| where english takes the prefix structure that places emphasis it the second one is the right-to-left decoding method which decodes in both of the left-to-right and right-to-left directions and merges the translation source and target texts .the above results could be justified by the structural difference of japanese and english , where english takes the prefix structure , setting attention around the end of a sentence .then , the section 3 presents decoding algorithms in three direction , left - to-right and bi-direction .the search problem of statistical machine translation is to induce the maximum likely channel model , 5 , 0 , 3 , 1 , 0 -rrb- figure 1 : an example of alignment for japanese and english sentences guage text -lrb- f -rrb- into a target language text -lrb- e -rrb- e the translation process is treated as a noisy channel model , like those used in speech recognition in which there exists e transcribed as f , and a translation is to the previous target word generated from the cept -lrb- j -rrb- .the position of a non-headthe decoding methods presented in this paper explore the partial candidate translation hypotheses greedily , as presented in tillmann and ney -lrb- 2000 -rrb- and och et al. .the algorithm assumes two kinds of partial hypotheses2 , translated partially from an input string , one is open hypothesis that can be extended by raising the fertility .the decoding algorithm generating in left-to-right direction fills the output sequence from the beginning of a sentence by consuming the input words in any order and by selecting the corresponding translation itsample japanese-to-english translations performed by comparing the translation model and lnguage model scores for the outputs from three decoding methods .table 3 shows the ratio of producing search errors , right-to-left and bidirectional method evaluated with wer , per and se .table 2 summarizes the results of decoding by left-to-right , right-to-left and bidirectional method evaluated with wer , 2002 -rrb- .the translation was carried out by three decoding methods : left-to-right , right-to - left and bidirectional .sample japanese-to-english translations performed by the decoders is presented in figure 6 .the left-to-right decoding method performed better than the right-to-left one in terms of wer/per scores , though the se score dropped from 8.7 % to 6.7 % in cranked sentences .table 2 : summary of results for japanese-to-english -lrb- j-e -rrb- translations by left-to-right -lrb- ltor -rrb- , right-to-left -lrb- rtol -rrb- and bidirectional -lrb- bi -rrb- decoding methods .nevertheless , the search error decreased from 59.3 into 34.0 by alternating the translation direction for the right-to-left decoding method , which still supports the use of correct rendering direction for translation target language . | There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.The search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given f and the model, P(fe) =La P(f, ae) and P(e).The skipping based criteria, such as introduced by Och et al.(2001), is not appropriate for the language pairs with drastically different alignment, such as Japanese and English, hence was not considered in this paper.The decoding algorithm generating in left-to-right direction fills the output sequence from the beginning of a sentence by consuming the input words in any order and by selecting the corresponding translation.Table 2 summarizes the results of decoding by left-to-right, right-to-left and bidirectional method evaluated with WER, PER and SE.The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection.0 57 .6 56 .1 58 .0 49 .3 % 10.0% 8.7% 32.0% 49 .3 % 10.0% 6.7% 34.0% 48 .7 % 8.0% 10.0% 33.3% input: suri ni saifu o sura re mashi ta (i had my pocket picked) LtoR: here ’s my wallet was stolen RtoL: here ’s my wallet was stolen Bi: i had my wallet stolen input: sumimasen ga terasu no seki ga ii no desu ga (excuse me but can we have a table on the terrace) LtoR: excuse me i ’d like a seat on the terrace RtoL: i ’d prefer excuse me Bi: i ’d like a seat on the terrace input: nan ji ni owaru no desu (what time will it be over) LtoR: what time should i be at the end RtoL: it ’s what time will it be over Bi : at what time is it end input: nimotsu o ue ni age te morae masu ka (will you put my luggage on the rack) LtoR: could you put my baggage here RtoL: do you have overhead luggage Bi: could you put my baggage input: ee ani to imouto ga hitori zutsu i masu (yes i have a brother and a sister) LtoR: yes brother and sister there a daughter RtoL: you ’re yes brother and sister daughter Bi: yes my daughter is there a brother and sister Figure 6: Examples of Japanese-to-English translation guage model employed for this experiment, for the language model probabilities were assigned based on the left history, not the right history. | The above results could be justified by the structural difference of Japanese and English, where English takes the prefix structure that places emphasis at the beginning of a sentence, hence prefers left-to-right decoding.On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.The former term, P(fe), is a translation model representing some correspondence between bilingual text.This problem is known to be NP-Complete (Knight, 1999), for the reordering property in the model further complicates the search.Again, the right-to-left direction is suitable for the language which enforces stronger constraints at the end of sentence, such as Japanese, similar to the reason mentioned above.The similar statement can hold for postfix languages, such as Japanese, where emphasis is placed around the end of a sentence.The translation models, both for the Japanese-to- English (J-E) and English-to-Japanese (E-J) translation, were trained toward IBM Model 4 on the training set and cross-validated on validation set to terminate the iteration by observing perplexity.The translation was carried out by three decoding methods:left-to-right, right-to- left and bidirectional one.The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection.0 57 .6 56 .1 58 .0 49 .3 % 10.0% 8.7% 32.0% 49 .3 % 10.0% 6.7% 34.0% 48 .7 % 8.0% 10.0% 33.3% input: suri ni saifu o sura re mashi ta (i had my pocket picked) LtoR: here ’s my wallet was stolen RtoL: here ’s my wallet was stolen Bi: i had my wallet stolen input: sumimasen ga terasu no seki ga ii no desu ga (excuse me but can we have a table on the terrace) LtoR: excuse me i ’d like a seat on the terrace RtoL: i ’d prefer excuse me Bi: i ’d like a seat on the terrace input: nan ji ni owaru no desu (what time will it be over) LtoR: what time should i be at the end RtoL: it ’s what time will it be over Bi : at what time is it end input: nimotsu o ue ni age te morae masu ka (will you put my luggage on the rack) LtoR: could you put my baggage here RtoL: do you have overhead luggage Bi: could you put my baggage input: ee ani to imouto ga hitori zutsu i masu (yes i have a brother and a sister) LtoR: yes brother and sister there a daughter RtoL: you ’re yes brother and sister daughter Bi: yes my daughter is there a brother and sister Figure 6: Examples of Japanese-to-English translation guage model employed for this experiment, for the language model probabilities were assigned based on the left history, not the right history. | This paper presents two decoding methods, one is the right-to-left decoding based on the left-to- right beam search algorithm, which generates outputs from the end of a sentence.Then, the Section 3 presents decoding algorithms in three direction, left- to-right, right-to-left and bi-direction.Statistical machine translation regards machine translation as a process of translating a source lan NULL0 could1 you2 recommend3 another4 hotel5 hoka no hoteru o shokaishi teitadake masu ka a = (4, 4, 5, 0, 3, 1, 1, 0) Figure 1: An example of alignment for Japanese and English sentences guage text (f) into a target language text (e) with the following formula: e = arg max P(e f) e The Bayes Rule is applied to the above to derive: e = arg max P(f e)P(e) e The translation process is treated as a noisy channel model, like those used in speech recognition in which there exists e transcribed as f, and a translation is to infer the best e from f in terms of P(fe)P(e).The algorithm assumes two kinds of partial hypotheses2, translated partially from an input string, one is an open hypothesis that can be extended by raising the fertility.Table 2 summarizes the results of decoding by left-to-right, right-to-left and bidirectional method evaluated with WER, PER and SE.Table 3 shows the ratio of producing search errors, computed by comparing the translation model and lnguage model scores for the outputs from three decoding methods.Table 2: Summary of results for Japanese-to-English (J-E) and English-to-Japanese (E-J) translations by left-to-right (LtoR), right-to-left (RtoL) and bidirectional (Bi) decoding methods.

